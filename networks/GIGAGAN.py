# https://github.com/lucidrains/gigagan-pytorch/blob/main/gigagan_pytorch/gigagan_pytorch.py

from math import log2
from functools import partial

import torch
import torch.nn.functional as F
from torch import nn, einsum
from torch.autograd import grad as torch_grad

from beartype import beartype
from beartype.typing import List, Optional, Tuple

from einops import rearrange, pack, unpack, repeat, reduce
from einops.layers.torch import Rearrange


# helpers

def exists(val):
    return val is not None

def default(*vals):
    for val in vals:
        if exists(val):
            return val
    return None

def is_power_of_two(n):
    return log2(n).is_integer()

# activation functions

def leaky_relu(neg_slope = 0.1):
    return nn.LeakyReLU(neg_slope)

def conv2d_3x3(dim_in, dim_out):
    return nn.Conv2d(dim_in, dim_out, 3, padding = 1)

# tensor helpers

def gradient_penalty(
    images,
    output,
    weight = 10
):
    batch_size = images.shape[0]
    gradients, *_ = torch_grad(
        outputs = output,
        inputs = images,
        grad_outputs = torch.ones_like(output),
        create_graph = True,
        retain_graph = True,
        only_inputs = True
    )

    gradients = rearrange(gradients, 'b ... -> b (...)')
    return weight * ((gradients.norm(2, dim=1) - 1) ** 2).mean()

# hinge gan losses
def gen_hinge_loss(fake):
    return fake.mean()

def hinge_loss(real, fake):
    return (F.relu(1 + real) + F.relu(1 - fake)).mean()


class ChannelRMSNorm(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.scale = dim ** 0.5
        self.gamma = nn.Parameter(torch.ones(dim, 1, 1))

    def forward(self, x):
        normed = F.normalize(x, dim=1)
        return normed * self.scale * self.gamma


class RMSNorm(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.scale = dim ** 0.5
        self.gamma = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        normed = F.normalize(x, dim=-1)
        return normed * self.scale * self.gamma


# down and upsample

class Upsample(nn.Module):
    def __init__(self, dim):
        super().__init__()
        conv = nn.Conv2d(dim, dim * 4, 1)

        self.net = nn.Sequential(
            conv,
            nn.SiLU(),
            nn.PixelShuffle(2)
        )

        self.init_conv_(conv)

    def init_conv_(self, conv):
        o, i, h, w = conv.weight.shape
        conv_weight = torch.empty(o // 4, i, h, w)
        nn.init.kaiming_uniform_(conv_weight)
        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')

        conv.weight.data.copy_(conv_weight)
        nn.init.zeros_(conv.bias.data)

    def forward(self, x):
        return self.net(x)


def Downsample(dim):
    return nn.Sequential(
        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1=2, s2=2),
        nn.Conv2d(dim * 4, dim, 1)
    )


# adaptive conv
# the main novelty of the paper - they propose to learn a softmax weighted sum of N convolutional kernels, depending on the text embedding

def get_same_padding(size, kernel, dilation, stride):
    return ((size - 1) * (stride - 1) + dilation * (kernel - 1)) // 2


class AdaptiveConv2DMod(nn.Module):
    def __init__(
            self,
            dim,
            dim_out,
            dim_embed,
            kernel,
            demod=True,
            stride=1,
            dilation=1,
            eps=1e-8,
            num_conv_kernels=1  # set this to be greater than 1 for adaptive
    ):
        super().__init__()
        self.eps = eps

        self.dim_out = dim_out

        self.kernel = kernel
        self.stride = stride
        self.dilation = dilation
        self.adaptive = num_conv_kernels > 1

        self.to_mod = nn.Linear(dim_embed, dim)
        self.to_adaptive_weight = nn.Linear(dim_embed, num_conv_kernels) if self.adaptive else None

        self.weights = nn.Parameter(torch.randn((num_conv_kernels, dim_out, dim, kernel, kernel)))

        self.demod = demod

        nn.init.kaiming_normal_(self.weights, a=0, mode='fan_in', nonlinearity='leaky_relu')

    def forward(self, fmap, embed):
        """
        notation
        b - batch
        n - convs
        o - output
        i - input
        k - kernel
        """

        b, h = fmap.shape[0], fmap.shape[-2]

        weights = self.weights

        if self.adaptive:
            weights = repeat(weights, '... -> b ...', b=b)

            # determine an adaptive weight and 'select' the kernel to use with softmax

            selections = self.to_adaptive_weight(embed).softmax(dim=-1)
            selections = rearrange(selections, 'b n -> b n 1 1 1 1')

            weights = reduce(weights * selections, 'b n ... -> b ...', 'sum')

        # do the modulation, demodulation, as done in stylegan2

        mod = self.to_mod(embed)

        mod = rearrange(mod, 'b i -> b 1 i 1 1')

        weights = weights * (mod + 1)

        if self.demod:
            inv_norm = reduce(weights ** 2, 'b o i k1 k2 -> b o 1 1 1', 'sum').clamp(min=self.eps).rsqrt()
            weights = weights * inv_norm

        fmap = rearrange(fmap, 'b c h w -> 1 (b c) h w')

        weights = rearrange(weights, 'b o ... -> (b o) ...')

        padding = get_same_padding(h, self.kernel, self.dilation, self.stride)
        fmap = F.conv2d(fmap, weights, padding=padding, groups=b)

        return rearrange(fmap, '1 (b o) ... -> b o ...', b=b)


# attention
# they use an attention with a better Lipchitz constant - l2 distance similarity instead of dot product - also shared query / key space - shown in vitgan to be more stable
# not sure what they did about token attention to self, so masking out, as done in some other papers using shared query / key space

class SelfAttention(nn.Module):
    def __init__(
            self,
            dim,
            dim_head=64,
            heads=8,
            mask_self_value=-1e2
    ):
        super().__init__()
        self.heads = heads
        self.scale = dim_head ** -0.5
        dim_inner = dim_head * heads

        self.mask_self_value = mask_self_value

        self.norm = ChannelRMSNorm(dim)
        self.to_qk = nn.Conv2d(dim, dim_inner, 1, bias=False)
        self.to_v = nn.Conv2d(dim, dim_inner, 1, bias=False)

        self.null_kv = nn.Parameter(torch.randn(2, heads, dim_head))

        self.to_out = nn.Conv2d(dim_inner, dim, 1, bias=False)

    def forward(self, fmap):
        """
        einstein notation
        b - batch
        h - heads
        x - height
        y - width
        d - dimension
        i - source seq (attend from)
        j - target seq (attend to)
        """
        batch, device = fmap.shape[0], fmap.device

        fmap = self.norm(fmap)

        x, y = fmap.shape[-2:]

        h = self.heads

        qk, v = self.to_qk(fmap), self.to_v(fmap)
        qk, v = map(lambda t: rearrange(t, 'b (h d) x y -> (b h) (x y) d', h=self.heads), (qk, v))

        q, k = qk, qk

        # add a null key / value, so network can choose to pay attention to nothing

        nk, nv = map(lambda t: repeat(t, 'h d -> (b h) 1 d', b=batch), self.null_kv)

        k = torch.cat((nk, k), dim=-2)
        v = torch.cat((nv, v), dim=-2)

        # l2 distance

        sim = -torch.cdist(q, k, p=2) * self.scale

        # following what was done in reformer for shared query / key space
        # omit attention to self

        self_mask = torch.eye(sim.shape[-2], device=device, dtype=torch.bool)
        self_mask = F.pad(self_mask, (1, 0), value=False)

        sim = sim.masked_fill(self_mask, self.mask_self_value)

        # attention

        attn = sim.softmax(dim=-1)

        out = einsum('b i j, b j d -> b i d', attn, v)

        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', x=x, y=y, h=h)

        return self.to_out(out)


class CrossAttention(nn.Module):
    def __init__(
            self,
            dim,
            dim_head=64,
            heads=8
    ):
        super().__init__()
        self.heads = heads
        self.scale = dim_head ** -0.5
        dim_inner = dim_head * heads

        self.norm = ChannelRMSNorm(dim)
        self.norm_context = RMSNorm(dim)

        self.to_q = nn.Conv2d(dim, dim_inner, 1, bias=False)
        self.to_kv = nn.Linear(dim, dim_inner * 2, bias=False)
        self.to_out = nn.Conv2d(dim_inner, dim, 1, bias=False)

    def forward(self, fmap, context):
        """
        einstein notation
        b - batch
        h - heads
        x - height
        y - width
        d - dimension
        i - source seq (attend from)
        j - target seq (attend to)
        """

        fmap = self.norm(fmap)
        context = self.norm_context(context[:, None, :])

        x, y = fmap.shape[-2:]

        h = self.heads

        q, k, v = (self.to_q(fmap), *self.to_kv(context).chunk(2, dim=-1))

        k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (k, v))

        q = rearrange(q, 'b (h d) x y -> (b h) (x y) d', h=self.heads)

        sim = -torch.cdist(q, k, p=2) * self.scale  # l2 distance

        attn = sim.softmax(dim=-1)

        out = einsum('b i j, b j d -> b i d', attn, v)

        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', x=x, y=y, h=h)

        return self.to_out(out)

class CrossAttention2(nn.Module):
    def __init__(
            self,
            dim,
            dim_head=64,
            heads=8
    ):
        super().__init__()
        self.heads = heads
        self.scale = dim_head ** -0.5
        dim_inner = dim_head * heads

        self.norm = ChannelRMSNorm(dim)
        self.norm_context = RMSNorm(dim)

        self.to_q = nn.Conv2d(dim, dim_inner, 1, bias=False)
        self.to_kv = nn.Linear(dim, dim_inner * 2, bias=False)
        self.to_out = nn.Conv2d(dim_inner, dim, 1, bias=False)

    def forward(self, fmap, context):
        """
        einstein notation
        b - batch
        h - heads
        x - height
        y - width
        d - dimension
        i - source seq (attend from)
        j - target seq (attend to)
        """

        fmap = self.norm(fmap)
        context = self.norm_context(context[:, None, :])

        x, y = fmap.shape[-2:]

        h = self.heads

        q, k, v = (self.to_q(fmap), *self.to_kv(context).chunk(2, dim=-1))

        k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (k, v))

        q = rearrange(q, 'b (h d) x y -> (b h) (x y) d', h=self.heads)

        sim = -torch.cdist(q, k, p=2) * self.scale  # l2 distance

        attn = sim.softmax(dim=1)

        out = einsum('b i j, b j d -> b i d', attn, v)

        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', x=x, y=y, h=h)

        return self.to_out(out)


class AudioAttn(nn.Module):
    def __init__(
            self,
            dim,
            dim_head=64,
            heads=8
    ):
        super().__init__()
        self.heads = heads
        self.scale = dim_head ** -0.5
        dim_inner = dim_head * heads

        self.norm = ChannelRMSNorm(dim)
        self.norm_context = RMSNorm(dim)

        self.to_q = nn.Conv2d(dim, dim_inner, 1, bias=False)
        self.to_v = nn.Linear(dim, dim_inner, bias=False)
        self.k = nn.Parameter(torch.randn(1, 1, dim_inner))
        self.to_out = nn.Conv2d(dim_inner, dim, 1, bias=False)

    def forward(self, fmap, context):
        """
        einstein notation
        b - batch
        h - heads
        x - height
        y - width
        d - dimension
        i - source seq (attend from)
        j - target seq (attend to)
        """

        fmap = self.norm(fmap)
        context = self.norm_context(context[:, None, :])

        x, y = fmap.shape[-2:]

        h = self.heads

        q, v = (self.to_q(fmap), self.to_v(context))
        k = self.k

        k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (k, v))

        q = rearrange(q, 'b (h d) x y -> (b h) (x y) d', h=self.heads)

        sim = -torch.cdist(q, k, p=2) * self.scale  # l2 distance

        attn = sim.softmax(dim=1)

        out = einsum('b i j, b j d -> b i d', attn, v)

        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', x=x, y=y, h=h)

        return self.to_out(out)

# classic transformer attention, stick with l2 distance

class TextAttention(nn.Module):
    def __init__(
            self,
            dim,
            dim_head=64,
            heads=8,
            mask_self_value=-1e2
    ):
        super().__init__()
        self.heads = heads
        self.scale = dim_head ** -0.5
        dim_inner = dim_head * heads

        self.mask_self_value = mask_self_value

        self.norm = RMSNorm(dim)
        self.to_qk = nn.Linear(dim, dim_inner, bias=False)
        self.to_v = nn.Linear(dim, dim_inner, bias=False)

        self.null_kv = nn.Parameter(torch.randn(2, heads, dim_head))

        self.to_out = nn.Linear(dim_inner, dim, bias=False)

    def forward(self, encodings, mask=None):
        """
        einstein notation
        b - batch
        h - heads
        x - height
        y - width
        d - dimension
        i - source seq (attend from)
        j - target seq (attend to)
        """
        batch, device = encodings.shape[0], encodings.device

        encodings = self.norm(encodings)

        h = self.heads

        qk, v = self.to_qk(encodings), self.to_v(encodings)
        qk, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=self.heads), (qk, v))

        q, k = qk, qk

        # add a null key / value, so network can choose to pay attention to nothing

        nk, nv = map(lambda t: repeat(t, 'h d -> (b h) 1 d', b=batch), self.null_kv)

        k = torch.cat((nk, k), dim=-2)
        v = torch.cat((nv, v), dim=-2)

        # l2 distance

        sim = -torch.cdist(q, k, p=2) * self.scale

        # following what was done in reformer for shared query / key space
        # omit attention to self

        self_mask = torch.eye(sim.shape[-2], device=device, dtype=torch.bool)
        self_mask = F.pad(self_mask, (1, 0), value=False)

        sim = sim.masked_fill(self_mask, self.mask_self_value)

        # key padding mask

        if exists(mask):
            mask = F.pad(mask, (1, 0), value=True)
            mask = repeat(mask, 'b n -> (b h) 1 n', h=h)
            sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)

        # attention

        attn = sim.softmax(dim=-1)
        out = einsum('b i j, b j d -> b i d', attn, v)

        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)

        return self.to_out(out)


# feedforward

def FeedForward(
        dim,
        mult=4,
        channel_first=False
):
    dim_hidden = int(dim * mult)
    norm_klass = ChannelRMSNorm if channel_first else RMSNorm
    proj = partial(nn.Conv2d, kernel_size=1) if channel_first else nn.Linear

    return nn.Sequential(
        norm_klass(dim),
        proj(dim, dim_hidden),
        nn.GELU(),
        proj(dim_hidden, dim)
    )


# different types of transformer blocks or transformers (multiple blocks)

class SelfAttentionBlock(nn.Module):
    def __init__(
            self,
            dim,
            dim_head=64,
            heads=8,
            ff_mult=4
    ):
        super().__init__()
        self.attn = SelfAttention(dim=dim, dim_head=dim_head, heads=heads)
        self.ff = FeedForward(dim=dim, mult=ff_mult, channel_first=True)

    def forward(self, x):
        x = self.attn(x) + x
        x = self.ff(x) + x
        return x

class CrossAttentionBlock(nn.Module):
    def __init__(
            self,
            dim,
            dim_head=64,
            heads=8,
            ff_mult=4
    ):
        super().__init__()
        self.attn = CrossAttention(dim=dim, dim_head=dim_head, heads=heads)
        self.ff = FeedForward(dim=dim, mult=ff_mult, channel_first=True)

    def forward(self, x, cond):
        x = self.attn(x, cond) + x
        x = self.ff(x) + x
        return x


class Transformer(nn.Module):
    def __init__(
            self,
            dim,
            depth,
            dim_head=64,
            heads=8,
            ff_mult=4
    ):
        super().__init__()
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                TextAttention(dim=dim, dim_head=dim_head, heads=heads),
                FeedForward(dim=dim, mult=ff_mult)
            ]))

        self.norm = RMSNorm(dim)

    def forward(self, x, mask=None):
        for attn, ff in self.layers:
            x = attn(x, mask=mask) + x
            x = ff(x) + x

        return self.norm(x)


# text encoder

class StyleNetwork(nn.Module):
    def __init__(
            self,
            dim,
            depth,
            dim_text_latent=0,
            frac_gradient=0.1
            # in the stylegan2 paper, they control the learning rate by multiplying the parameters by a constant, but we can use another trick here from attention literature
    ):
        super().__init__()

        layers = []
        for i in range(depth):
            layers.extend([nn.Linear(dim + dim_text_latent, dim), leaky_relu()])

        self.net = nn.Sequential(*layers)
        self.frac_gradient = frac_gradient
        self.dim_text_latent = dim_text_latent

    def forward(
            self,
            x,
            text_latent=None
    ):
        grad_frac = self.frac_gradient

        if self.dim_text_latent:
            assert exists(text_latent)
            x = torch.cat((x, text_latent), dim=-1)

        x = F.normalize(x, dim=1)
        out = self.net(x)

        return out * grad_frac + (1 - grad_frac) * out.detach()


# generator

class Generator(nn.Module):
    def __init__(
            self,
            *,
            dim,
            image_size,
            dim_max=8192,
            style_network: Optional[StyleNetwork] = None,
            dim_latent=512
    ):
        super().__init__()
        self.dim = dim
        self.style_network = style_network
        self.text_encoder = text_encoder

        assert is_power_of_two(image_size)
        num_layers = int(log2(image_size) - 1)
        self.num_layers = num_layers

        self.init_block = nn.Parameter(torch.randn(dim_latent, 4, 4))

    @property
    def device(self):
        return next(self.parameters()).device

    def forward(
            self,
            noise=None,
            styles=None,
            texts: Optional[List[str]] = None,
            global_text_tokens=None,
            fine_text_tokens=None,
            text_mask=None,
            batch_size=1
    ):
        # take care of text encodings
        # which requires global text tokens to adaptively select the kernels from the main contribution in the paper
        # and fine text tokens to attend to using cross attention

        if exists(texts):
            assert exists(self.text_encoder)
            global_text_tokens, fine_text_tokens, text_mask = self.text_encoder(texts)
        else:
            assert all([*map(exists, (global_text_tokens, fine_text_tokens, text_mask))])

        # determine styles

        if not exists(styles):
            assert exists(self.style_network)
            noise = default(noise, torch.randn((batch_size, self.dim), device=self.device))
            styles = self.style_network(noise)

        # main network

        return styles


# discriminator

class Discriminator(nn.Module):
    def __init__(
            self,
            *,
            dim,
            image_size,
            capacity=16,
            dim_max=8192,
            channels=3,
            attn_resolutions: Tuple[int] = (32, 16),
            attn_dim_head=64,
            attn_heads=8,
            ff_mult=4
    ):
        super().__init__()
        assert is_power_of_two(image_size)
        assert all([*map(is_power_of_two, attn_resolutions)])

        num_layers = int(log2(image_size) - 1)
        self.num_layers = num_layers

        resolutions = image_size / ((2 ** torch.arange(num_layers)))
        resolutions = resolutions.long().tolist()

        dim_layers = (2 ** (torch.arange(num_layers) + 1)) * capacity
        dim_layers = F.pad(dim_layers, (1, 0), value=channels)
        dim_layers.clamp_(max=dim_max)

        dim_layers = dim_layers.tolist()
        dim_last = dim_layers[-1]
        dim_pairs = list(zip(dim_layers[:-1], dim_layers[1:]))

        self.residual_scale = 2 ** -0.5
        self.layers = nn.ModuleList([])

        for ind, ((dim_in, dim_out), resolution) in enumerate(zip(dim_pairs, resolutions)):
            is_first = ind == 0
            is_last = (ind + 1) == len(dim_pairs)
            should_downsample = not is_last
            has_attn = resolution in attn_resolutions

            residual_conv = nn.Conv2d(dim_in, dim_out, 1, stride=(2 if should_downsample else 1))

            resnet_block = nn.Sequential(
                conv2d_3x3(dim_in, dim_out),
                leaky_relu(),
                conv2d_3x3(dim_out, dim_out),
                leaky_relu()
            )

            self.layers.append(nn.ModuleList([
                resnet_block,
                residual_conv,
                SelfAttentionBlock(dim_out, heads=attn_heads, dim_head=attn_dim_head,
                                   ff_mult=ff_mult) if has_attn else None,
                Downsample(dim_out) if should_downsample else None,
            ]))

        self.to_logits = nn.Sequential(
            conv2d_3x3(dim_last, dim_last),
            Rearrange('b c h w -> b (c h w)'),
            nn.Linear(dim_last * (4 ** 2), 1),
            Rearrange('b 1 -> b')
        )

    def forward(
            self,
            images,
            texts: Optional[List[str]] = None
    ):
        x = images

        for block, residual_fn, attn, downsample in self.layers:
            residual = residual_fn(x)
            x = block(x)

            if exists(attn):
                x = attn(x)

            if exists(downsample):
                x = downsample(x)

            x = x + residual
            x = x * self.residual_scale

        return self.to_logits(x)
